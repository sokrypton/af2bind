{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sokrypton/af2bind/blob/main/af2bind_large_pdb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L6J9pl1XqIf"
      },
      "source": [
        "### AF2BIND: Prediction of ligand-binding sites using AlphaFold2\n",
        "\n",
        "AF2BIND is a simple and fast notebook that runs inference on the output obtained from [AlphaFold2](https://github.com/deepmind/alphafold).\n",
        "\n",
        "<!--<img src=\"https://raw.githubusercontent.com/artemg97/af2bind_prod/main/logo.png\" width=\"300\">.-->\n",
        "\n",
        "<figure>\n",
        "<img src='https://raw.githubusercontent.com/artemg97/af2bind_prod/main/logo.png'  width=\"300\" height=\"150\"/>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "For more details see preprint:\n",
        "\n",
        "**AF2BIND: Predicting ligand-binding sites using the pair representation of AlphaFold2**\n",
        "* Artem Gazizov, Anna Lian, Casper Alexander Goverde, Sergey Ovchinnikov, Nicholas F. Polizzi\n",
        "* https://doi.org/10.1101/2023.10.15.562410\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "k7_AcQqYaCHk"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "#@title Install AlphaFold2 (~2 mins)\n",
        "#@markdown Please execute this cell by pressing the *Play* button on\n",
        "#@markdown the left.\n",
        "\n",
        "!pip install bio\n",
        "!pip install biopython\n",
        "\n",
        "import os, time\n",
        "if not os.path.isdir(\"params\"):\n",
        "  # get code\n",
        "  print(\"installing ColabDesign\")\n",
        "  os.system(\"(mkdir params; apt-get install aria2 -qq; \\\n",
        "  aria2c -q -x 16 https://storage.googleapis.com/alphafold/alphafold_params_2021-07-14.tar; \\\n",
        "  mkdir af2bind_params; \\\n",
        "  wget -qnc https://github.com/sokrypton/af2bind/raw/main/attempt_7_2k_lam0-03.zip; unzip attempt_7_2k_lam0-03.zip -d af2bind_params; \\\n",
        "  tar -xf alphafold_params_2021-07-14.tar -C params; touch params/done.txt )&\")\n",
        "\n",
        "  os.system(\"pip -q install git+https://github.com/sokrypton/ColabDesign.git@v1.1.1\")\n",
        "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabdesign colabdesign\")\n",
        "\n",
        "  # download params\n",
        "  if not os.path.isfile(\"params/done.txt\"):\n",
        "    print(\"downloading params\")\n",
        "    while not os.path.isfile(\"params/done.txt\"):\n",
        "      time.sleep(5)\n",
        "\n",
        "import gc\n",
        "from colabdesign import mk_afdesign_model, clear_mem\n",
        "from IPython.display import HTML\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "\n",
        "from colabdesign.af.alphafold.common import residue_constants\n",
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "data_table._DEFAULT_FORMATTERS[float] = lambda x: f\"{x:.3f}\"\n",
        "from IPython.display import display, HTML\n",
        "import jax, pickle\n",
        "import jax.numpy as jnp\n",
        "from scipy.special import expit as sigmoid\n",
        "import plotly.express as px\n",
        "\n",
        "import py3Dmol\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.special import softmax\n",
        "import copy\n",
        "from colabdesign.shared.protein import renum_pdb_str\n",
        "from colabdesign.af.alphafold.common import protein\n",
        "\n",
        "\n",
        "aa_order = {v:k for k,v in residue_constants.restype_order.items()}\n",
        "\n",
        "def get_pdb(pdb_code=\"\"):\n",
        "  if pdb_code is None or pdb_code == \"\":\n",
        "    upload_dict = files.upload()\n",
        "    pdb_string = upload_dict[list(upload_dict.keys())[0]]\n",
        "    with open(\"tmp.pdb\",\"wb\") as out: out.write(pdb_string)\n",
        "    return \"tmp.pdb\"\n",
        "  elif os.path.isfile(pdb_code):\n",
        "    return pdb_code\n",
        "  elif len(pdb_code) == 4:\n",
        "    os.system(f\"wget -qnc https://files.rcsb.org/view/{pdb_code}.pdb\")\n",
        "    return f\"{pdb_code}.pdb\"\n",
        "  else:\n",
        "    os.system(f\"wget -qnc https://alphafold.ebi.ac.uk/files/AF-{pdb_code}-F1-model_v4.pdb\")\n",
        "    return f\"AF-{pdb_code}-F1-model_v4.pdb\"\n",
        "\n",
        "def af2bind(outputs, mask_sidechains=True, seed=0):\n",
        "  pair_A = outputs[\"representations\"][\"pair\"][:-20,-20:]\n",
        "  pair_B = outputs[\"representations\"][\"pair\"][-20:,:-20].swapaxes(0,1)\n",
        "  pair_A = pair_A.reshape(pair_A.shape[0],-1)\n",
        "  pair_B = pair_B.reshape(pair_B.shape[0],-1)\n",
        "  x = np.concatenate([pair_A,pair_B],-1)\n",
        "\n",
        "  # get params\n",
        "  if mask_sidechains:\n",
        "    model_type = f\"split_nosc_pair_A_split_nosc_pair_B_{seed}\"\n",
        "  else:\n",
        "    model_type = f\"split_pair_A_split_pair_B_{seed}\"\n",
        "  with open(f\"af2bind_params/attempt_7_2k_lam0-03/{model_type}.pickle\",\"rb\") as handle:\n",
        "    params_ = pickle.load(handle)\n",
        "  params_ = dict(**params_[\"~\"], **params_[\"linear\"])\n",
        "  p = jax.tree_map(lambda x:np.asarray(x), params_)\n",
        "\n",
        "  # get predictions\n",
        "  x = (x - p[\"mean\"]) / p[\"std\"]\n",
        "  x = (x * p[\"w\"][:,0]) + (p[\"b\"] / x.shape[-1])\n",
        "  p_bind_aa = x.reshape(x.shape[0],2,20,-1).sum((1,3))\n",
        "  p_bind = sigmoid(p_bind_aa.sum(-1))\n",
        "  return {\"p_bind\":p_bind, \"p_bind_aa\":p_bind_aa}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wzcddcfVHhv-"
      },
      "outputs": [],
      "source": [
        "#@title **Set up a pipeline for large PDBs ðŸ§¬ (~ 2 mins)**\n",
        "#@markdown - Install Merizo and define the functions for subgraph search (optional)\n",
        "import ipywidgets as widgets\n",
        "from Bio import PDB\n",
        "import os\n",
        "import shutil\n",
        "from collections import ChainMap\n",
        "from Bio.PDB import PDBParser, PDBIO, Select\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "import warnings\n",
        "import csv\n",
        "import re\n",
        "import io\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "if not os.path.isdir(\"Merizo\"):\n",
        "  os.system(\"git  clone https://github.com/psipred/Merizo\")\n",
        "  os.system(\"pip install -r Merizo/requirements.txt\")\n",
        "  os.system(\"pip install torch torchvision torchaudio scipy matplotlib einops networkx rotary-embedding-torch natsort\")\n",
        "\n",
        "\n",
        "\n",
        "def setup_directory(directory_path):\n",
        "    # Check if the directory exists\n",
        "    if os.path.exists(directory_path):\n",
        "        # If it exists, remove it\n",
        "        shutil.rmtree(directory_path)\n",
        "\n",
        "    # Create the directory\n",
        "    os.makedirs(directory_path)\n",
        "    print(f\"## Directory '{directory_path}' created.\")\n",
        "\n",
        "\n",
        "def split_pdb_by_chains(input_pdb, chain_ids):\n",
        "\n",
        "  \"\"\"\n",
        "  Splits a PDB file into separate files based on provided chain IDs.\n",
        "\n",
        "  Parameters:\n",
        "  input_pdb (str): The path to the input PDB file.\n",
        "  chain_ids (list): A list of chain IDs (e.g. ['A', 'B']).\n",
        "\n",
        "  Returns:\n",
        "  None: Writes new PDB files for each chain.\n",
        "  \"\"\"\n",
        "\n",
        "  single_chain_pdbs_list=[]\n",
        "\n",
        "  chain_ids = chain_ids.split(',')\n",
        "  save_dir=f\"{SINGLE_CHAINS_PATH}/{input_pdb}\"\n",
        "  setup_directory(save_dir)\n",
        "\n",
        "  # Read the input PDB file\n",
        "  with open(input_pdb+\".pdb\", 'r') as pdb_file:\n",
        "      pdb_lines = pdb_file.readlines()\n",
        "\n",
        "  # Create a dictionary to store the lines for each chain\n",
        "  chain_data = {chain: [] for chain in chain_ids}\n",
        "\n",
        "  # Iterate through each line and assign to the appropriate chain\n",
        "  for line in pdb_lines:\n",
        "      if line.startswith(('ATOM', 'HETATM')):  # Only process ATOM and HETATM records\n",
        "          chain_id = line[21]  # Chain identifier is at column 22 (index 21 in 0-based index)\n",
        "          if chain_id in chain_data:\n",
        "              chain_data[chain_id].append(line)\n",
        "\n",
        "  # Write out each chain to a new file\n",
        "  for chain in chain_ids:\n",
        "      output_filename = f\"{save_dir}/chain_{chain}.pdb\"\n",
        "      with open(output_filename, 'w') as output_file:\n",
        "          output_file.writelines(chain_data[chain])\n",
        "      print(f\"Chain {chain} written to {output_filename}\")\n",
        "      single_chain_pdbs_list.append(output_filename)\n",
        "\n",
        "\n",
        "  return single_chain_pdbs_list\n",
        "\n",
        "\n",
        "def extract_residues(pdb_path, residue_range, chain_id, output_name,domain_n):\n",
        "    # Create a PDB parser\n",
        "    parser = PDB.PDBParser(QUIET=True)\n",
        "\n",
        "    # Parse the structure\n",
        "    structure = parser.get_structure(\"protein\", pdb_path)\n",
        "\n",
        "    # Prepare a set to store selected residues\n",
        "    selected_residues = []\n",
        "\n",
        "    # Convert the residue range to a list of integers\n",
        "    ranges = residue_range.split('_')\n",
        "    residue_ids = set()\n",
        "\n",
        "    for r in ranges:\n",
        "        # Check if it's a single residue or a range\n",
        "        if '-' in r:\n",
        "            start, end = map(int, r.split('-'))\n",
        "            residue_ids.update(range(start, end + 1))\n",
        "        else:\n",
        "            # It's a single residue\n",
        "            residue_ids.add(int(r))\n",
        "\n",
        "    # Iterate through the structure to collect selected residues\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            # Check if the current chain matches the specified chain ID\n",
        "            if chain.id == chain_id:\n",
        "                for residue in chain:\n",
        "                    # Check if the residue ID is in the selected range\n",
        "                    if residue.id[1] in residue_ids:  # residue.id[1] is the residue number\n",
        "                        selected_residues.append(residue)\n",
        "\n",
        "    # Write the selected residues to a new PDB file\n",
        "    io = PDB.PDBIO()\n",
        "    io.set_structure(structure)\n",
        "\n",
        "    # Create a new structure to save selected residues\n",
        "    new_structure = PDB.Structure.Structure('selected_residues')\n",
        "    new_model = PDB.Model.Model(0)\n",
        "    new_chain = PDB.Chain.Chain(chain_id)  # Use the specified chain ID\n",
        "\n",
        "    for residue in selected_residues:\n",
        "        new_chain.add(residue)\n",
        "\n",
        "    new_model.add(new_chain)\n",
        "    new_structure.add(new_model)\n",
        "\n",
        "    # Save to a new PDB file with the specified output name\n",
        "    output_file = output_name  # Use the specified output name\n",
        "    io.set_structure(new_structure)\n",
        "    io.save(output_file)\n",
        "\n",
        "    print(\"\")\n",
        "    print(f'## domain {chain_id}{domain_n} saved to {output_file}')\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "\n",
        "def get_domains(target_pdb, single_chain_pdbs_list):\n",
        "    my_domains_pdb_path = {}\n",
        "    my_domains_res_range = {}\n",
        "\n",
        "    # Run merizo on each single chain\n",
        "    for chain_path in single_chain_pdbs_list:\n",
        "        print(\"\")\n",
        "        print(f\"{target_pdb}, chain {chain_path} is being processed\")\n",
        "\n",
        "        chain = chain_path.split(\"/\")[-1].split(\"chain_\")[-1][0]\n",
        "\n",
        "        # Run domain separation for a particular chain\n",
        "        !python Merizo/predict.py -i {chain_path} --pdb_chain {chain} --save_domains > /dev/null\n",
        "\n",
        "        # Get domain info path for a particular chain\n",
        "        domain_info_path = os.path.join(SINGLE_CHAINS_PATH, target_pdb, f\"chain_{chain}_merizo_v2.domains\")\n",
        "\n",
        "        with open(domain_info_path, 'r') as file:\n",
        "            # For each domain create a pdb\n",
        "            for line in file:\n",
        "\n",
        "                domain_n = line.strip().split(\"\\t\")[-2]\n",
        "                domain_r = line.strip().split(\"\\t\")[-1]\n",
        "\n",
        "                current_domain_out_path = os.path.join(DOMAINS_PATH, target_pdb, f\"chain_{chain}_domain_{domain_n}.pdb\")\n",
        "                extract_residues(chain_path, domain_r, chain, current_domain_out_path,domain_n)\n",
        "                key_n = chain + domain_n\n",
        "                my_domains_pdb_path[key_n] = current_domain_out_path\n",
        "\n",
        "                my_arr = []\n",
        "                for d in domain_r.split(\"_\"):\n",
        "                    my_arr.append([int(n) for n in d.split(\"-\")])\n",
        "\n",
        "                my_domains_res_range[key_n] = {\"res\": my_arr}\n",
        "\n",
        "    return my_domains_res_range, my_domains_pdb_path\n",
        "\n",
        "def get_heavy_atom_coordinates(pdb_filename):\n",
        "    \"\"\"\n",
        "    Retrieve coordinates of all heavy atoms from a PDB file.\n",
        "    \"\"\"\n",
        "    parser = PDBParser()\n",
        "    structure = parser.get_structure(\"protein\", pdb_filename)\n",
        "    coords = {}\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                res_id = residue.get_id()\n",
        "                if res_id[0] == ' ':\n",
        "                    res_coords = []\n",
        "                    for atom in residue:\n",
        "                        if atom.element != 'H':  # Exclude hydrogen atoms\n",
        "                            res_coords.append(atom.get_coord())\n",
        "                    coords[(chain.id, res_id[1])] = res_coords\n",
        "    return coords\n",
        "\n",
        "\n",
        "def calculate_distance(coord1, coord2):\n",
        "    \"\"\"\n",
        "    Calculate the Euclidean distance between two 3D coordinates.\n",
        "    \"\"\"\n",
        "    return sum((c1 - c2) ** 2 for c1, c2 in zip(coord1, coord2)) ** 0.5\n",
        "\n",
        "\n",
        "\n",
        "def find_contacting_domains(all_chains_domains, input_pdb_file, threshold=5.0, min_contact_residues=5):\n",
        "    \"\"\"\n",
        "    Find contacting domains for each domain based on distance threshold and residue count.\n",
        "\n",
        "    Parameters:\n",
        "    - all_chains_domains: List of dictionaries mapping domains to residue ranges for each chain.\n",
        "    - input_pdb_file: Path to the input PDB file containing 3D coordinates.\n",
        "    - threshold: Distance threshold to consider residues as in contact.\n",
        "    - min_contact_residues: Minimum number of contacting residues required to consider two domains in contact.\n",
        "\n",
        "    Returns:\n",
        "    - contacting_domains: Dictionary where keys are domain identifiers and values are sets of contacting domains.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get heavy atom coordinates from PDB file\n",
        "    coords = get_heavy_atom_coordinates(input_pdb_file)\n",
        "\n",
        "    # Initialize contacting domains dictionary\n",
        "    contacting_domains = {domain: set() for chain_domains in all_chains_domains for domain in chain_domains}\n",
        "\n",
        "    # Flatten list of domain dictionaries into a single dictionary\n",
        "    chain_domains = dict(ChainMap(*all_chains_domains))\n",
        "\n",
        "    # Iterate over each pair of domains\n",
        "    for domain1, data1 in chain_domains.items():\n",
        "        for domain2, data2 in chain_domains.items():\n",
        "            if domain1 != domain2:\n",
        "                contact_count = 0  # Count of contacting residues between domain1 and domain2\n",
        "\n",
        "                # Check each residue pair from domain1 and domain2\n",
        "                for res_interval_1 in data1['res']:\n",
        "                    for res_interval_2 in data2['res']:\n",
        "                        for res_1 in range(res_interval_1[0], res_interval_1[1] + 1):\n",
        "                            if (domain1[:-1], res_1) not in coords:\n",
        "                                continue\n",
        "                            for res_2 in range(res_interval_2[0], res_interval_2[1] + 1):\n",
        "                                if (domain2[:-1], res_2) not in coords:\n",
        "                                    continue\n",
        "\n",
        "                                res1_coords = coords[(domain1[:-1], res_1)]\n",
        "                                res2_coords = coords[(domain2[:-1], res_2)]\n",
        "\n",
        "                                # Check distances between all pairs of coordinates\n",
        "                                for coord1 in res1_coords:\n",
        "                                    for coord2 in res2_coords:\n",
        "                                        distance = calculate_distance(coord1, coord2)\n",
        "                                        if distance <= threshold:\n",
        "                                            contact_count += 1\n",
        "                                            break  # No need to check other coordinates once contact is found\n",
        "                                    if contact_count >= min_contact_residues:\n",
        "                                        break  # Break if minimum contact residues threshold is reached\n",
        "                                if contact_count >= min_contact_residues:\n",
        "                                    break  # Break if minimum contact residues threshold is reached\n",
        "                            if contact_count >= min_contact_residues:\n",
        "                                break  # Break if minimum contact residues threshold is reached\n",
        "                        if contact_count >= min_contact_residues:\n",
        "                            break  # Break if minimum contact residues threshold is reached\n",
        "\n",
        "                # If enough contacts are found, mark domains as contacting each other\n",
        "                if contact_count >= min_contact_residues:\n",
        "                    contacting_domains[domain1].add(domain2)\n",
        "                    contacting_domains[domain2].add(domain1)\n",
        "\n",
        "    return contacting_domains\n",
        "\n",
        "\n",
        "\n",
        "def find_residues_contacting_domains(all_chains_domains, input_pdb_file, threshold=5.0):\n",
        "    \"\"\"\n",
        "    Find contacting residues between domains for each domain based on a distance threshold.\n",
        "    \"\"\"\n",
        "    pdb_filename = input_pdb_file\n",
        "    coords = get_heavy_atom_coordinates(pdb_filename)\n",
        "\n",
        "    # Initialize contacting residues dictionary for each domain\n",
        "    contacting_residues = {domain: [] for chain_domains in all_chains_domains for domain in chain_domains}\n",
        "\n",
        "    # Create a mapping for chain domains\n",
        "    chain_domains = dict(ChainMap(*all_chains_domains))\n",
        "\n",
        "    # Track domain pairs that have already been compared\n",
        "    compared_pairs = set()\n",
        "\n",
        "    # Iterate over all domain pairs (domain1 and domain2)\n",
        "    for domain1, data1 in chain_domains.items():\n",
        "        for domain2, data2 in chain_domains.items():\n",
        "            # Skip if comparing the same domain or if this domain pair has already been compared\n",
        "            if domain1 == domain2 or (domain1, domain2) in compared_pairs or (domain2, domain1) in compared_pairs:\n",
        "                continue\n",
        "\n",
        "            # Mark this domain pair as compared\n",
        "            compared_pairs.add((domain1, domain2))\n",
        "\n",
        "            # Iterate over residue intervals in both domains\n",
        "            for res_interval_1 in data1['res']:\n",
        "                for res_interval_2 in data2['res']:\n",
        "                    for res_1 in range(res_interval_1[0], res_interval_1[1] + 1):\n",
        "                        # Check if the coordinates for residue res_1 in domain1 exist\n",
        "                        if (domain1[:-1], res_1) not in coords:\n",
        "                            continue\n",
        "                        for res_2 in range(res_interval_2[0], res_interval_2[1] + 1):\n",
        "                            # Check if the coordinates for residue res_2 in domain2 exist\n",
        "                            if (domain2[:-1], res_2) not in coords:\n",
        "                                continue\n",
        "                            # Get coordinates for both residues\n",
        "                            res1_coords = coords[(domain1[:-1], res_1)]\n",
        "                            res2_coords = coords[(domain2[:-1], res_2)]\n",
        "\n",
        "                            # Check if any atom pairs between res1 and res2 are within the threshold distance\n",
        "                            for coord1 in res1_coords:\n",
        "                                for coord2 in res2_coords:\n",
        "                                    distance = calculate_distance(coord1, coord2)\n",
        "                                    if distance <= threshold:\n",
        "                                        # Save both residues as contacting residues for their respective domains\n",
        "                                        contacting_residues[domain1].append(res_1)\n",
        "                                        contacting_residues[domain2].append(res_2)\n",
        "                                        break  # No need to check further atom pairs once a contact is found\n",
        "\n",
        "    # Remove duplicate residue indices and sort them for each domain\n",
        "    for domain in contacting_residues:\n",
        "        contacting_residues[domain] = sorted(set(contacting_residues[domain]))\n",
        "\n",
        "    return contacting_residues\n",
        "\n",
        "\n",
        "def get_pdb_length(pdb_file_path):\n",
        "    parser = PDB.PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure('structure', pdb_file_path)\n",
        "\n",
        "    residues = set()\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            for residue in chain:\n",
        "                residues.add((chain.id, residue.id[1]))\n",
        "\n",
        "    return len(residues)\n",
        "\n",
        "\n",
        "def process_graph(contacting_domains, all_chains_domains_paths, max_sum, threshold):\n",
        "    # Create a graph from contacting_domains\n",
        "    G = nx.Graph()\n",
        "\n",
        "    # Add nodes with values\n",
        "    for node in contacting_domains:\n",
        "        G.add_node(node, value=get_pdb_length(all_chains_domains_paths[node]))\n",
        "\n",
        "    # Add edges\n",
        "    for node, neighbors in contacting_domains.items():\n",
        "        for neighbor in neighbors:\n",
        "            G.add_edge(node, neighbor)\n",
        "\n",
        "    def get_all_connected_subgraphs(graph):\n",
        "        nodes = list(graph.nodes())\n",
        "        subgraphs = []\n",
        "\n",
        "        # Generate all possible subgraphs\n",
        "        for size in range(1, len(nodes) + 1):\n",
        "            for node_combination in combinations(nodes, size):\n",
        "                subgraph = graph.subgraph(node_combination)\n",
        "                if nx.is_connected(subgraph):\n",
        "                    subgraphs.append(subgraph)\n",
        "\n",
        "        return subgraphs\n",
        "\n",
        "    def filter_and_sort_subgraphs(subgraphs, max_sum):\n",
        "        valid_subgraphs = []\n",
        "        seen_subgraphs = []\n",
        "\n",
        "        # Sort subgraphs by their size and total value in descending order\n",
        "        subgraphs.sort(key=lambda sg: (len(sg.nodes()), sum(nx.get_node_attributes(sg, 'value').values())), reverse=True)\n",
        "\n",
        "        for subgraph in subgraphs:\n",
        "            node_values = nx.get_node_attributes(subgraph, 'value')\n",
        "            total_value = sum(node_values.values())\n",
        "\n",
        "            subgraph_nodes = set(subgraph.nodes())\n",
        "\n",
        "            # Check if subgraph_nodes is a subset of any seen subgraph nodes\n",
        "            if any(subgraph_nodes <= seen for seen in seen_subgraphs):\n",
        "                continue\n",
        "\n",
        "            if total_value <= max_sum:\n",
        "                valid_subgraphs.append((subgraph, total_value))\n",
        "                seen_subgraphs.append(subgraph_nodes)\n",
        "\n",
        "        return valid_subgraphs\n",
        "\n",
        "\n",
        "    def find_pairs_with_value_greater_than(graph, threshold):\n",
        "        pairs = []\n",
        "\n",
        "        for node1, node2 in graph.edges():\n",
        "            value1 = graph.nodes[node1]['value']\n",
        "            value2 = graph.nodes[node2]['value']\n",
        "            total_value = value1 + value2\n",
        "\n",
        "            if total_value > threshold:\n",
        "                pairs.append((node1, node2))\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def find_single_domains_with_value_greater_than(graph, threshold):\n",
        "        nodes = list(graph.nodes())\n",
        "        high_value_nodes = []\n",
        "\n",
        "        for node in nodes:\n",
        "            value = graph.nodes[node]['value']\n",
        "            if value > threshold:\n",
        "                high_value_nodes.append(node)\n",
        "\n",
        "        return high_value_nodes\n",
        "\n",
        "    # Get all connected subgraphs\n",
        "    all_connected_subgraphs = get_all_connected_subgraphs(G)\n",
        "\n",
        "    # Filter subgraphs by the sum of their node attributes and sort them\n",
        "    filtered_sorted_subgraphs = filter_and_sort_subgraphs(all_connected_subgraphs, max_sum)\n",
        "\n",
        "    my_batch = []\n",
        "    contacting_domain_combinations=[]\n",
        "    # Collect all valid subgraphs\n",
        "    for subgraph, total_value in filtered_sorted_subgraphs:\n",
        "        nodes = subgraph.nodes(data=False)\n",
        "\n",
        "        my_batch.append(list(nodes))\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Subgraph has following domains:\", nodes)\n",
        "        print(\"Total length of subgraph:\", total_value)\n",
        "        print(\"\")\n",
        "\n",
        "    # Find pairs with value greater than the threshold\n",
        "    pairs_greater_th = find_pairs_with_value_greater_than(G, threshold)\n",
        "\n",
        "    for pairs in pairs_greater_th:\n",
        "        my_batch.append(list(pairs))\n",
        "        print(\"\")\n",
        "        print(\"Pairs with length greater than 300 res. :\",pairs)\n",
        "        print(\"\")\n",
        "\n",
        "    # Find single domains with value greater than the threshold\n",
        "    single_greater_th = find_single_domains_with_value_greater_than(G, threshold)\n",
        "    print(\"\")\n",
        "    print(\"Single domains greater than 300 res.:\",single_greater_th)\n",
        "    print(\"\")\n",
        "\n",
        "    print(my_batch)\n",
        "    for domains in my_batch:\n",
        "        contacting_domain_combinations.append(\"_\".join(domains))\n",
        "\n",
        "    return contacting_domain_combinations, single_greater_th\n",
        "\n",
        "def copy_pdb(true_pdb_path, output_pdb_path):\n",
        "    \"\"\"\n",
        "    Copies a PDB file from true_pdb_path to output_pdb_path.\n",
        "\n",
        "    Parameters:\n",
        "    true_pdb_path (str): The path to the source PDB file.\n",
        "    output_pdb_path (str): The path to the destination PDB file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        shutil.copyfile(true_pdb_path, output_pdb_path)\n",
        "        print(f\"File copied from {true_pdb_path} to {output_pdb_path}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "def merge_pdb_files(output_file, *input_files):\n",
        "    # Initialize a PDB writer\n",
        "    pdb_writer = PDB.PDBIO()\n",
        "    # Create a structure object to hold the merged structure\n",
        "    merged_structure = PDB.Structure.Structure('merged_structure')\n",
        "\n",
        "    # Create a model to hold the merged data\n",
        "    model = PDB.Model.Model(0)\n",
        "    merged_structure.add(model)\n",
        "\n",
        "    # Dictionary to accumulate residues for each chain ID\n",
        "    chains_dict = {}\n",
        "\n",
        "    # Iterate over each input PDB file\n",
        "    for pdb_file in input_files:\n",
        "        parser = PDB.PDBParser(QUIET=True)\n",
        "        structure = parser.get_structure('temp_structure', pdb_file)\n",
        "\n",
        "        # Extract chains from the current structure\n",
        "        for chain in structure.get_chains():\n",
        "            chain_id = chain.id\n",
        "\n",
        "            # Initialize a new list for this chain if it doesn't exist\n",
        "            if chain_id not in chains_dict:\n",
        "                chains_dict[chain_id] = []\n",
        "\n",
        "            # Collect all residues for the current chain\n",
        "            for residue in chain.get_residues():\n",
        "                # Create a copy of the residue and add it to the list\n",
        "                new_residue = residue.copy()\n",
        "                chains_dict[chain_id].append(new_residue)\n",
        "\n",
        "    # Now create chains in the model based on the collected residues\n",
        "    for chain_id, residues in chains_dict.items():\n",
        "        # Create a new chain\n",
        "        new_chain = PDB.Chain.Chain(chain_id)\n",
        "        for residue in residues:\n",
        "            new_chain.add(residue)\n",
        "\n",
        "        # Add the new chain to the model\n",
        "        model.add(new_chain)\n",
        "\n",
        "    # Set the merged structure to the writer and save it to the output file\n",
        "    pdb_writer.set_structure(merged_structure)\n",
        "    pdb_writer.save(output_file)\n",
        "\n",
        "def save_set_to_csv(s, filename):\n",
        "    \"\"\"\n",
        "    Save a set of integers to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    s (set): The set of integers to save.\n",
        "    filename (str): The name of the CSV file to save the data in.\n",
        "    \"\"\"\n",
        "    # Convert set to a sorted list (optional, if you want sorted output)\n",
        "    data = sorted(s)\n",
        "\n",
        "    # Open the file in write mode\n",
        "    with open(filename, 'w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "\n",
        "        # Write each number on a new row\n",
        "        for number in data:\n",
        "            writer.writerow([number])\n",
        "\n",
        "\n",
        "def save_dict_to_csv(data, filename):\n",
        "    \"\"\"\n",
        "    Save a dictionary to a CSV file with keys as columns and list elements as rows.\n",
        "\n",
        "    Parameters:\n",
        "    data (dict): The dictionary to save. Keys should be column names and values should be lists of row values.\n",
        "    filename (str): The name of the file to save the CSV as.\n",
        "    \"\"\"\n",
        "    # Find the maximum length of the lists\n",
        "    max_len = max(len(v) for v in data.values())\n",
        "\n",
        "    # Prepare the rows\n",
        "    rows = []\n",
        "    for i in range(max_len):\n",
        "        row = []\n",
        "        for key in data:\n",
        "            row.append(data[key][i] if i < len(data[key]) else '')\n",
        "        rows.append(row)\n",
        "\n",
        "    # Write to CSV\n",
        "    with open(filename, 'w', newline='') as csv_file:\n",
        "        writer = csv.writer(csv_file)\n",
        "        writer.writerow(data.keys())  # Write the header\n",
        "        writer.writerows(rows)        # Write the data rows\n",
        "\n",
        "\n",
        "\n",
        "def create_or_replace_dir(directory):\n",
        "    if os.path.exists(directory):  # Check if directory exists\n",
        "        print(f\"Directory '{directory}' already exists. Deleting and creating again.\")\n",
        "        try:\n",
        "            shutil.rmtree(directory)  # Remove existing directory and its contents\n",
        "        except OSError as e:\n",
        "            print(f\"Error: {directory} : {e.strerror}\")\n",
        "        os.makedirs(directory)  # Create directory\n",
        "    else:\n",
        "        print(f\"Directory '{directory}' does not exist. Creating...\")\n",
        "        os.makedirs(directory)  # Create directory\n",
        "\n",
        "\n",
        "# Function to update B-factors and save the structure\n",
        "def update_b_factors_and_save(structure, df, output_path):\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            chain_id = chain.get_id()\n",
        "            for residue in chain:\n",
        "                res_id = residue.get_id()[1]\n",
        "                # Check if both chain and residue match\n",
        "                if ((df['chain'] == chain_id) & (df['resi'] == res_id)).any():\n",
        "                    b_factor = df.loc[(df['chain'] == chain_id) & (df['resi'] == res_id), 'p(bind)'].values[0]\n",
        "                    #print(f\"Chain {chain_id} Residue {res_id}: Setting B-factor to {b_factor}\")\n",
        "                else:\n",
        "                    b_factor = 0.0\n",
        "                    #print(f\"Chain {chain_id} Residue {res_id}: Setting B-factor to {b_factor} (default)\")\n",
        "                for atom in residue:\n",
        "\n",
        "                    atom.set_bfactor(b_factor*100)\n",
        "\n",
        "    # Save the updated structure\n",
        "    class BFactorSelect(Select):\n",
        "        def accept_atom(self, atom):\n",
        "            return True\n",
        "\n",
        "    pdb_io = PDBIO()\n",
        "    pdb_io.set_structure(structure)\n",
        "    pdb_io.save(output_path, BFactorSelect())\n",
        "    print(f\"Structure saved to {output_path}\")\n",
        "\n",
        "def remove_residues(input_pdb_path, output_pdb_path, residues_to_remove):\n",
        "    \"\"\"\n",
        "    Remove specified residues from a PDB file and save the result to a new PDB file.\n",
        "\n",
        "    Parameters:\n",
        "    - input_pdb_path: Path to the input PDB file.\n",
        "    - output_pdb_path: Path to save the modified PDB file.\n",
        "    - residues_to_remove: Set of residue indices to be removed.\n",
        "    \"\"\"\n",
        "    # Create a parser and structure object\n",
        "    parser = PDB.PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure('structure', input_pdb_path)\n",
        "\n",
        "    # Create a writer object\n",
        "    io = PDB.PDBIO()\n",
        "    io.set_structure(structure)\n",
        "\n",
        "    # Define a function to filter out residues\n",
        "    class ResidueSelector(PDB.Select):\n",
        "        def __init__(self, residues_to_remove):\n",
        "            self.residues_to_remove = residues_to_remove\n",
        "\n",
        "        def accept_residue(self, residue):\n",
        "            # Check if residue index is in the set of residues to remove\n",
        "            return residue.get_id()[1] not in self.residues_to_remove\n",
        "\n",
        "    # Create a ResidueSelector instance\n",
        "    residue_selector = ResidueSelector(residues_to_remove)\n",
        "\n",
        "    # Save the structure with selected residues\n",
        "    io.save(output_pdb_path, residue_selector)\n",
        "\n",
        "\n",
        "def af2bind_inference(pdb_filename, target_chain, mask_sidechains=True, mask_sequence=False, preds_pdb_path=\"\",residues_ignore=[]):\n",
        "\n",
        "    mask_sidechains=True\n",
        "    mask_sequence=False\n",
        "\n",
        "    print(f\"\\n# running af2bind on: {pdb_filename}\")\n",
        "    print(f\"# chain(s): {target_chain}\")\n",
        "\n",
        "    clear_mem()\n",
        "    \n",
        "    af_model = mk_afdesign_model(protocol=\"binder\", debug=True)\n",
        "    af_model.prep_inputs(pdb_filename=pdb_filename,\n",
        "                         chain=target_chain,\n",
        "                         binder_len=20,\n",
        "                         rm_target_sc=mask_sidechains,\n",
        "                         rm_target_seq=mask_sequence)\n",
        "\n",
        "    # split\n",
        "    r_idx = af_model._inputs[\"residue_index\"][-20] + (1 + np.arange(20)) * 50\n",
        "    af_model._inputs[\"residue_index\"][-20:] = r_idx.flatten()\n",
        "\n",
        "    af_model.set_seq(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "    af_model.predict(verbose=False)\n",
        "\n",
        "    o = af2bind(af_model.aux[\"debug\"][\"outputs\"],\n",
        "                mask_sidechains=mask_sidechains)\n",
        "    pred_bind = o[\"p_bind\"].copy()\n",
        "    pred_bind_aa = o[\"p_bind_aa\"].copy()\n",
        "\n",
        "    #######################################################\n",
        "    #labels = [\"chain\", \"resi\", \"resn\", \"p(bind)\"]\n",
        "    labels = [\"chain\",\"resi\",\"resn\",\"p(bind)\",\"arr_i\"]\n",
        "    data = []\n",
        "    for i in range(af_model._target_len):\n",
        "        c = af_model._pdb[\"idx\"][\"chain\"][i]\n",
        "        r = af_model._pdb[\"idx\"][\"residue\"][i]\n",
        "        a = aa_order.get(af_model._pdb[\"batch\"][\"aatype\"][i], \"X\")\n",
        "        p = pred_bind[i]\n",
        "        #data.append([c, r, a, p])\n",
        "        data.append([c,r,a,p,i])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=labels)\n",
        "    df_sorted = df.sort_values(\"p(bind)\", ascending=False, ignore_index=True).rename_axis('rank').reset_index()\n",
        "\n",
        "    if (len(residues_ignore)>0):\n",
        "\n",
        "        #print(residues_ignore)\n",
        "        arr_i_list = df_sorted.loc[df_sorted['resi'].isin(residues_ignore), 'arr_i'].tolist()\n",
        "        for index in arr_i_list:\n",
        "            pred_bind[index] = 0\n",
        "\n",
        "    preds_adj = pred_bind.copy()\n",
        "\n",
        "    L = af_model._target_len\n",
        "    aux = copy.deepcopy(af_model.aux[\"all\"])\n",
        "    aux[\"plddt\"][:, :L] = preds_adj\n",
        "    out_name = pdb_filename.split(\"/\")[-1].split(\".pdb\")[0] + \"_pred.pdb\"\n",
        "    #af_model.save_pdb(f\"{preds_pdb_path}/{out_name}\", aux={\"all\": aux})\n",
        "\n",
        "    aux[\"atom_mask\"][:,L:] = 0\n",
        "    x = {k:[] for k in [\"aatype\",\n",
        "                      \"residue_index\",\n",
        "                      \"atom_positions\",\n",
        "                      \"atom_mask\",\n",
        "                      \"b_factors\"]}\n",
        "    asym_id = []\n",
        "    for i in range(af_model._target_len):\n",
        "        for k in [\"aatype\",\"atom_mask\"]: x[k].append(aux[k][0,i])\n",
        "\n",
        "        x[\"atom_positions\"].append(af_model._pdb[\"batch\"][\"all_atom_positions\"][i])\n",
        "        x[\"residue_index\"].append(af_model._pdb[\"idx\"][\"residue\"][i])\n",
        "        x[\"b_factors\"].append(x[\"atom_mask\"][-1] * aux[\"plddt\"][0,i] * 100.0)\n",
        "        asym_id.append(af_model._pdb[\"idx\"][\"chain\"][i])\n",
        "    x = {k:np.array(v) for k,v in x.items()}\n",
        "\n",
        "    # fix the chains\n",
        "    (n,resnum_) = (0,None)\n",
        "    pdb_lines = []\n",
        "    for line in protein.to_pdb(protein.Protein(**x)).splitlines():\n",
        "        if line[:4] == \"ATOM\":\n",
        "          resnum = int(line[22:22+5])\n",
        "          if resnum_ is None: resnum_ = resnum\n",
        "          if resnum != resnum_:\n",
        "            n += 1\n",
        "            resnum_ = resnum\n",
        "          pdb_lines.append(\"%s%s%4i%s\" % (line[:21],asym_id[n],resnum,line[26:]))\n",
        "\n",
        "    with open(f\"{preds_pdb_path}/{out_name}\",\"w\") as handle:\n",
        "        handle.write(\"\\n\".join(pdb_lines))\n",
        "\n",
        "\n",
        "    return df_sorted\n",
        "\n",
        "def convert_filenames(filename):\n",
        "    # Use regular expression to extract the domain part from the filename\n",
        "    match = re.search(r'_([A-Z]\\d)(_[A-Z]\\d)?\\.pdb$', filename)\n",
        "\n",
        "    if match:\n",
        "        # If there's a second domain part, it's a combination\n",
        "        if match.group(2):\n",
        "            return \"\"\n",
        "        else:\n",
        "            return match.group(1)\n",
        "    return \"\"\n",
        "\n",
        "def get_low_bfactor_stretches(pdb_filename, bfactor_threshold=70.0, min_stretch_length=7):\n",
        "    \"\"\"\n",
        "    Extract contiguous stretches of residues with B-factors below the threshold\n",
        "    Parameters:\n",
        "    pdb_filename (str): Path to the PDB file.\n",
        "    bfactor_threshold (float): The B-factor threshold. Default is 70.0.\n",
        "    min_stretch_length (int): Minimum length of contiguous stretches to be stored. Default is 7.\n",
        "\n",
        "    Returns:\n",
        "    list: List of lists where each sublist represents a contiguous stretch of residues with B-factors below the threshold.\n",
        "    \"\"\"\n",
        "    # Read and parse the PDB file\n",
        "    with open(pdb_filename, 'r') as f:\n",
        "        pdb_content = f.read()\n",
        "\n",
        "    pdb_io = io.StringIO(pdb_content)\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure('structure', pdb_io)\n",
        "\n",
        "    def get_low_bfactor_residues_from_chain(chain):\n",
        "        residues = []\n",
        "        for residue in chain:\n",
        "            res_id = residue.get_id()[1]  # Get the residue sequence number\n",
        "            b_factors = [atom.get_bfactor() for atom in residue]\n",
        "            if any(b_factor < bfactor_threshold for b_factor in b_factors):\n",
        "                residues.append(res_id)\n",
        "            else:\n",
        "                if len(residues) >= min_stretch_length:\n",
        "                    yield residues\n",
        "                residues = []\n",
        "\n",
        "        if len(residues) >= min_stretch_length:\n",
        "            yield residues\n",
        "\n",
        "    def collect_from_terminal(chain, start_residue, direction):\n",
        "        residues = []\n",
        "        residue_dict = {residue.get_id()[1]: residue for residue in chain.get_residues()}\n",
        "        current_residue = start_residue\n",
        "        while current_residue:\n",
        "            res_id = current_residue.get_id()[1]  # Get the residue sequence number\n",
        "            b_factors = [atom.get_bfactor() for atom in current_residue]\n",
        "            if any(b_factor < bfactor_threshold for b_factor in b_factors):\n",
        "                residues.append(res_id)\n",
        "            else:\n",
        "                break\n",
        "            # Move to the next residue\n",
        "            next_res_id = res_id + 1 if direction == 'N' else res_id - 1\n",
        "            current_residue = residue_dict.get(next_res_id, None)\n",
        "        return residues\n",
        "\n",
        "    low_bfactor_stretches = []\n",
        "\n",
        "    # Iterate over all models, chains, and residues\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            residues = list(chain.get_residues())\n",
        "            if residues:\n",
        "                # Collect residues starting from N-terminus and C-terminus\n",
        "                n_terminal_stretch = collect_from_terminal(chain, residues[0], 'N')\n",
        "                if len(n_terminal_stretch) >= min_stretch_length:\n",
        "                    low_bfactor_stretches.append(n_terminal_stretch)\n",
        "\n",
        "                c_terminal_stretch = collect_from_terminal(chain, residues[-1], 'C')\n",
        "                if len(c_terminal_stretch) >= min_stretch_length:\n",
        "                    low_bfactor_stretches.append(c_terminal_stretch)\n",
        "\n",
        "                # Collect and process all stretches of residues with B-factors below threshold\n",
        "                for stretch in get_low_bfactor_residues_from_chain(chain):\n",
        "                    low_bfactor_stretches.append(stretch)\n",
        "\n",
        "    return low_bfactor_stretches\n",
        "\n",
        "\n",
        "def af2bind_pipeline(preds_path, target_chain, out_process_pdb_file, pdb_filepath, low_bfactor_residues):\n",
        "\n",
        "    try:\n",
        "\n",
        "        pdb_i=out_process_pdb_file[\"pdb_i\"]\n",
        "\n",
        "        # Inference: create folder for af2bind inference pdb\n",
        "        preds_pdb_path = preds_path + \"/\" + out_process_pdb_file[\"pdb_i\"]\n",
        "        create_or_replace_dir(preds_pdb_path)\n",
        "        print(\"\")\n",
        "\n",
        "        if(af2_struct):\n",
        "          pdb_filepath_b_high=preds_pdb_path + \"/\" + out_process_pdb_file[\"pdb_i\"] +  \"_high_pLDDT.pdb\"\n",
        "          remove_residues(input_pdb_path=pdb_filepath, output_pdb_path=pdb_filepath_b_high, residues_to_remove=low_bfactor_residues)\n",
        "\n",
        "\n",
        "        # Iterate through pdbs of single domains & combination of multiple domains\n",
        "        all_df = []\n",
        "\n",
        "        for pdb_filename in out_process_pdb_file[\"all_domains_af2bind_run\"]:\n",
        "\n",
        "            #remove low plddt streches and update the existing files\n",
        "            remove_residues(input_pdb_path=pdb_filename, output_pdb_path=pdb_filename, residues_to_remove=flattened_low_bfactor_stretches)\n",
        "\n",
        "            # Get domain name like \"A1\" from for example 3hlg_A_merizo_v2_01.pdb\n",
        "            domain_name_single = convert_filenames(pdb_filename)\n",
        "            #print(\"domain name \", domain_name_single)\n",
        "            residues_ignore = []\n",
        "\n",
        "            # Get contacting residues for single domains\n",
        "            if domain_name_single != \"\":  # meaning if it is a single domain, since convert_filenames will output \"\" for contacting domains\n",
        "\n",
        "                print(f\"## single domain {domain_name_single}, contacting residues will be zeroed!\")\n",
        "                residues_ignore = out_process_pdb_file[\"contacting_residues\"][domain_name_single]\n",
        "                print(\"## ignored contacting residues: \",residues_ignore)\n",
        "\n",
        "\n",
        "            df_sorted = af2bind_inference(pdb_filename, target_chain, mask_sidechains=True, mask_sequence=False, preds_pdb_path=preds_pdb_path, residues_ignore=residues_ignore)\n",
        "\n",
        "\n",
        "\n",
        "            # Set contacting residues to 0 for single domains\n",
        "            df_sorted.loc[df_sorted['resi'].isin(residues_ignore), 'p(bind)'] = 0\n",
        "\n",
        "            all_df.append(df_sorted.copy())\n",
        "\n",
        "            # Full domain name like e.g. 3hlg_A_merizo_v2_01.pdb\n",
        "            domain_name = pdb_filename.split(\"/\")[-1].split(\".pdb\")[0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Lumping all the predictions\n",
        "        concatenated_df = pd.concat(all_df, ignore_index=True)\n",
        "        grouped = concatenated_df.groupby(['chain', 'resi'])\n",
        "        max_pbind_df = grouped['p(bind)'].max().reset_index()\n",
        "        sorted_pbind_df = max_pbind_df.sort_values(by='p(bind)', ascending=False)\n",
        "\n",
        "        sorted_pbind_df.to_csv(f\"{preds_pdb_path}/domains_lumped_{pdb_i}.csv\", index=False)\n",
        "\n",
        "        #metrics = calculate_rec_avpbind(sorted_pbind_df, binding_sites)\n",
        "        #print(\"av_pbind \", metrics[\"av_pbind\"])\n",
        "        #print(\"recovery \", metrics[\"recovery\"])\n",
        "\n",
        "        print(\"n. res: \", len(sorted_pbind_df))\n",
        "\n",
        "        pdb_len = len(df_sorted)\n",
        "\n",
        "        # Save lumped pdb file\n",
        "        pdb_parser = PDBParser(QUIET=True)\n",
        "        if(af2_struct):\n",
        "          structure = pdb_parser.get_structure('protein', pdb_filepath_b_high)\n",
        "        else:\n",
        "          structure = pdb_parser.get_structure('protein', out_process_pdb_file[\"input_pdb_file\"])\n",
        "\n",
        "        out_name = out_process_pdb_file[\"input_pdb_file\"].split(\"/\")[-1].split(\".pdb\")[0] + \"_lumped_pred.pdb\"\n",
        "        update_b_factors_and_save(structure, sorted_pbind_df, f\"{preds_pdb_path}/{out_name}\")\n",
        "\n",
        "\n",
        "        #save contacting residues\n",
        "        save_dict_to_csv(data=out_process_pdb_file[\"contacting_residues\"],filename=f'{preds_pdb_path}/contacting_residues.csv')\n",
        "\n",
        "\n",
        "        #save removed low plddt streches\n",
        "        save_set_to_csv(s=low_bfactor_residues,filename=f'{preds_pdb_path}/low_plddt_streches.csv')\n",
        "\n",
        "\n",
        "\n",
        "        print(\"### end ####\")\n",
        "        print(\"\")\n",
        "\n",
        "    except ValueError as e:\n",
        "        print(f\"ValueError occurred: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def process_pdb_file(target_pdb, chain_ids):\n",
        "    \"\"\"\n",
        "    Process the PDB file to extract domain-related information and create\n",
        "    PDBs for contacting domains.\n",
        "\n",
        "    Parameters:\n",
        "        target_pdb (str): The target PDB file.\n",
        "        chain_ids (list): List of chain IDs to process.\n",
        "\n",
        "    Returns:\n",
        "        dict: Contains information about processed PDBs and contacting residues.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Split PDB by chains\n",
        "    print(\"## Splitting PDB by chains....\")\n",
        "    single_chain_pdbs_list = split_pdb_by_chains(input_pdb=target_pdb, chain_ids=chain_ids)\n",
        "\n",
        "    # Step 2: Get domain information and residue ranges\n",
        "    print(\"## Getting domain residue ranges and paths....\")\n",
        "    all_chains_domains_residues, all_chains_domains_paths = get_domains(target_pdb, single_chain_pdbs_list)\n",
        "    all_chains_domains_residues = [all_chains_domains_residues]\n",
        "\n",
        "    # Step 3: Find contacting domains\n",
        "    print(\"## Finding contacting domains....\")\n",
        "    contacting_domains = find_contacting_domains(all_chains_domains_residues, f\"{target_pdb}.pdb\")\n",
        "    print(f\"Contacting domains: {contacting_domains}\\n\")\n",
        "\n",
        "    # Step 4: Find contacting residues in domains\n",
        "    print(\"## Finding contacting residues in contacting domains....\")\n",
        "    contacting_residues = find_residues_contacting_domains(all_chains_domains_residues, f\"{target_pdb}.pdb\")\n",
        "\n",
        "    # Step 5: Process the contact graph and get domain subgraphs\n",
        "    print(\"## Processing contact graph....\")\n",
        "    contacting_domain_combinations, single_greater_than_th = process_graph(\n",
        "        contacting_domains,\n",
        "        all_chains_domains_paths,\n",
        "        MAX_SUM_RES_SUBGRAPH,\n",
        "        THRESHOLD_SINGLE_PAIRS\n",
        "    )\n",
        "\n",
        "    # Step 6: Create PDBs for contacting domains\n",
        "    print(\"## Creating PDBs for contacting domains....\")\n",
        "    all_domains_af2bind_run = create_contacting_pdbs(\n",
        "        target_pdb,\n",
        "        contacting_domain_combinations,\n",
        "        all_chains_domains_paths,\n",
        "        single_greater_than_th\n",
        "    )\n",
        "\n",
        "    print(\"Done!\")\n",
        "\n",
        "    return {\n",
        "        \"all_domains_af2bind_run\": all_domains_af2bind_run,\n",
        "        \"pdb_i\": target_pdb,\n",
        "        \"contacting_residues\": contacting_residues,\n",
        "        \"input_pdb_file\": f\"{target_pdb}.pdb\"\n",
        "    }\n",
        "\n",
        "\n",
        "def create_contacting_pdbs(target_pdb, contacting_domain_combinations, all_chains_domains_paths, single_greater_than_th):\n",
        "    \"\"\"\n",
        "    Create PDBs by merging domains and adding single domains above a threshold.\n",
        "\n",
        "    Parameters:\n",
        "        target_pdb (str): The target PDB file.\n",
        "        contacting_domain_combinations (list): List of contacting domain combinations.\n",
        "        all_chains_domains_paths (dict): Mapping of domain names to PDB paths.\n",
        "        single_greater_than_th (list): Single domains with a size greater than a threshold.\n",
        "\n",
        "    Returns:\n",
        "        list: Paths to the created PDB files.\n",
        "    \"\"\"\n",
        "    all_domains_af2bind_run = []\n",
        "\n",
        "    # Create merged PDBs for contacting domain combinations\n",
        "    for contact_domain in contacting_domain_combinations:\n",
        "        domains = contact_domain.split(\"_\")\n",
        "        contact_domains_name = f\"{target_pdb}_\" + \"_\".join(domains) + \".pdb\"\n",
        "        output_file = f\"{CONTACTING_DOMAINS}/{target_pdb}/{contact_domains_name}\"\n",
        "        merge_pdb_files(output_file, *[all_chains_domains_paths[domain] for domain in domains])\n",
        "        all_domains_af2bind_run.append(output_file)\n",
        "\n",
        "    # Add single domains greater than threshold\n",
        "    for domain in single_greater_than_th:\n",
        "        single_domain_name = f\"{target_pdb}_{domain}.pdb\"\n",
        "        output_file = f\"{CONTACTING_DOMAINS}/{target_pdb}/{single_domain_name}\"\n",
        "        copy_pdb(all_chains_domains_paths[domain], output_file)\n",
        "        all_domains_af2bind_run.append(output_file)\n",
        "\n",
        "    print(f\"Single domains greater than threshold: {single_greater_than_th}\")\n",
        "    return all_domains_af2bind_run\n",
        "\n",
        "\n",
        "def remove_residues_from_pdb(pdb_path, residues_to_remove, output_path):\n",
        "    \"\"\"\n",
        "    Removes specified residues from a PDB file and saves the modified structure.\n",
        "\n",
        "    Parameters:\n",
        "    pdb_path (str): Path to the input PDB file.\n",
        "    residues_to_remove (set): Set of residues to be removed. Format: {1, 2, 3, 4, 5, ...}.\n",
        "                              Example: {100, 50} or {1, 2, 3, 4, 5}\n",
        "    output_path (str): Path to save the modified PDB file.\n",
        "    \"\"\"\n",
        "    # Parse the input PDB file\n",
        "    parser = PDBParser(QUIET=True)\n",
        "    structure = parser.get_structure(\"structure\", pdb_path)\n",
        "\n",
        "    # Iterate through all chains and residues, removing specified residues\n",
        "    for model in structure:\n",
        "        for chain in model:\n",
        "            chain_id = chain.id\n",
        "            residues_to_keep = []\n",
        "            for residue in chain:\n",
        "                res_id = residue.id[1]  # Residue ID (integer part)\n",
        "                if res_id not in residues_to_remove:\n",
        "                    residues_to_keep.append(residue)\n",
        "\n",
        "            # Update the chain's residues to only keep the desired ones\n",
        "            chain.child_list = residues_to_keep\n",
        "\n",
        "    # Save the modified structure\n",
        "    io = PDBIO()\n",
        "    io.set_structure(structure)\n",
        "    io.save(output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "MNnPNFkOFL1N"
      },
      "outputs": [],
      "source": [
        "#@title **Run AF2Bind ðŸ”¬**\n",
        "\n",
        "# Globals\n",
        "global flattened_low_bfactor_stretches\n",
        "global MAX_SUM_RES_SUBGRAPH\n",
        "global THRESHOLD_SINGLE_PAIRS\n",
        "global af2_struct\n",
        "# Subgraph search parameters for large PDBs\n",
        "MAX_SUM_RES_SUBGRAPH = 300 # max sum for all subgraphs\n",
        "THRESHOLD_SINGLE_PAIRS = 300 # threshold for single domains & pairs of domains\n",
        "\n",
        "# Paths\n",
        "SINGLE_CHAINS_PATH=\"af2bind_temp/single_chains\"\n",
        "DOMAINS_PATH=\"af2bind_temp/domains\"\n",
        "CONTACTING_DOMAINS=\"af2bind_temp/contacting_domains\"\n",
        "PREDS_PATH=\"af2bind_temp/preds\"\n",
        "\n",
        "# Inputs\n",
        "target_pdb = \"7LQ6\" #@param {type:\"string\"}\n",
        "target_chain = \"A\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown - Please indicate target pdb (or uniprot ID to download from AlphaFoldDB) and chain.\n",
        "#@markdown - Leave pdb blank for custom upload prompt.\n",
        "large_pdb = True # @param {\"type\":\"boolean\"}\n",
        "#@markdown - (optional) For large PDBs (>300 res), we offer a separate pipeline that splits the PDB into domains and finds the optimal combinations to amplify the signal.\n",
        "af2_struct = False # @param {\"type\":\"boolean\"}\n",
        "#@markdown - (optional) For large AF2-predicted structures (>300 res), low pLDDT regions can be removed.\n",
        "\n",
        "mask_sidechains = True\n",
        "mask_sequence = False\n",
        "\n",
        "target_pdb = target_pdb.replace(\" \",\"\")\n",
        "target_chain = target_chain.replace(\" \",\"\")\n",
        "if target_chain == \"\":\n",
        "  target_chain = \"A\"\n",
        "\n",
        "pdb_filename = get_pdb(target_pdb)\n",
        "print(pdb_filename)\n",
        "if(target_pdb==\"\"):\n",
        "  target_pdb=\"tmp\"\n",
        "\n",
        "\n",
        "# Prepare Temporary Directories\n",
        "try:\n",
        "  os.makedirs(\"af2bind_temp\", exist_ok=True)\n",
        "  os.makedirs(SINGLE_CHAINS_PATH, exist_ok=True)\n",
        "  os.makedirs(DOMAINS_PATH, exist_ok=True)\n",
        "  os.makedirs(CONTACTING_DOMAINS, exist_ok=True)\n",
        "  os.makedirs(PREDS_PATH, exist_ok=True)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "#delete the directory, if it already exists\n",
        "setup_directory(f\"{DOMAINS_PATH}/{target_pdb}\")\n",
        "setup_directory(f\"{SINGLE_CHAINS_PATH}/{target_pdb}\")\n",
        "setup_directory(f\"{CONTACTING_DOMAINS}/{target_pdb}\")\n",
        "\n",
        "\n",
        "if(large_pdb):\n",
        "\n",
        "  print(\"###\")\n",
        "  print(\"## The large PDB pipeline is applied\")\n",
        "  print(\"###\\n\")\n",
        "\n",
        "\n",
        "  out_process_pdb_file= process_pdb_file(target_pdb,target_chain)\n",
        "\n",
        "  if af2_struct:\n",
        "    low_bfactor_stretches = get_low_bfactor_stretches(out_process_pdb_file[\"input_pdb_file\"])\n",
        "  else:\n",
        "    low_bfactor_stretches = set()\n",
        "\n",
        "  flattened_low_bfactor_stretches = set(item for sublist in low_bfactor_stretches for item in sublist)\n",
        "\n",
        "  af2bind_pipeline(\n",
        "      PREDS_PATH,\n",
        "      target_chain,\n",
        "      out_process_pdb_file,\n",
        "      out_process_pdb_file[\"input_pdb_file\"],\n",
        "      flattened_low_bfactor_stretches\n",
        "  )\n",
        "\n",
        "  csv_file_path=f\"{PREDS_PATH}/{target_pdb}/domains_lumped_{target_pdb}.csv\"\n",
        "  df = pd.read_csv(csv_file_path)\n",
        "  df['rank'] = range(len(df))\n",
        "  df = df[['rank'] + [col for col in df.columns if col != 'rank']]  # Ensures 'rank' is the first column\n",
        "  df.to_csv('results.csv')\n",
        "  data_table.enable_dataframe_formatter()\n",
        "  df_sorted = df.sort_values(\"p(bind)\",ascending=False, ignore_index=True)\n",
        "  display(data_table.DataTable(df_sorted, min_width=100, num_rows_per_page=15, include_index=False))\n",
        "\n",
        "  top_n = 15\n",
        "  top_n_idx = df['p(bind)'].argsort()[::-1][:top_n]  # Get indices of the top N p(bind) values\n",
        "  pymol_cmd = \"select ch\" + str(target_chain) + \",\"\n",
        "\n",
        "  for n, i in enumerate(top_n_idx):\n",
        "      # Extracting values directly from the DataFrame\n",
        "      p = df['p(bind)'].iloc[i]  # p(bind) value\n",
        "      c = df['chain'].iloc[i]       # Chain value\n",
        "      r = df['resi'].iloc[i]        # Residue value\n",
        "\n",
        "      pymol_cmd += f\" resi {r}\"\n",
        "      if n < top_n - 1:\n",
        "          pymol_cmd += \" +\"\n",
        "\n",
        "  print(\"\\nðŸ§ª Pymol Selection Cmd:\")\n",
        "  print(pymol_cmd)\n",
        "\n",
        "  #clear the cache\n",
        "  gc.collect()\n",
        "  jax.clear_caches()\n",
        "\n",
        "else:\n",
        "\n",
        "  if af2_struct:\n",
        "    low_bfactor_stretches = get_low_bfactor_stretches(pdb_filename)\n",
        "    flattened_low_bfactor_stretches = set(item for sublist in low_bfactor_stretches for item in sublist)\n",
        "    remove_residues_from_pdb(pdb_filename,flattened_low_bfactor_stretches,pdb_filename)\n",
        "\n",
        "  else:\n",
        "    low_bfactor_stretches = set()\n",
        "\n",
        "\n",
        "  clear_mem()\n",
        "  af_model = mk_afdesign_model(protocol=\"binder\", debug=True)\n",
        "  af_model.prep_inputs(pdb_filename=pdb_filename,\n",
        "                      chain=target_chain,\n",
        "                      binder_len=20,\n",
        "                      rm_target_sc=mask_sidechains,\n",
        "                      rm_target_seq=mask_sequence)\n",
        "\n",
        "  # split\n",
        "  r_idx = af_model._inputs[\"residue_index\"][-20] + (1 + np.arange(20)) * 50\n",
        "  af_model._inputs[\"residue_index\"][-20:] = r_idx.flatten()\n",
        "\n",
        "  af_model.set_seq(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "  af_model.predict(verbose=False)\n",
        "\n",
        "  o = af2bind(af_model.aux[\"debug\"][\"outputs\"],\n",
        "              mask_sidechains=mask_sidechains)\n",
        "  pred_bind = o[\"p_bind\"].copy()\n",
        "  pred_bind_aa = o[\"p_bind_aa\"].copy()\n",
        "\n",
        "  #######################################################\n",
        "  labels = [\"chain\",\"resi\",\"resn\",\"p(bind)\"]\n",
        "  data = []\n",
        "  for i in range(af_model._target_len):\n",
        "    c = af_model._pdb[\"idx\"][\"chain\"][i]\n",
        "    r = af_model._pdb[\"idx\"][\"residue\"][i]\n",
        "    a = aa_order.get(af_model._pdb[\"batch\"][\"aatype\"][i],\"X\")\n",
        "    p = pred_bind[i]\n",
        "    data.append([c,r,a,p])\n",
        "\n",
        "  df = pd.DataFrame(data, columns=labels)\n",
        "  df.to_csv('results.csv')\n",
        "\n",
        "  data_table.enable_dataframe_formatter()\n",
        "  df_sorted = df.sort_values(\"p(bind)\",ascending=False, ignore_index=True).rename_axis('rank').reset_index()\n",
        "  display(data_table.DataTable(df_sorted, min_width=100, num_rows_per_page=15, include_index=False))\n",
        "\n",
        "  top_n = 15\n",
        "  top_n_idx = pred_bind.argsort()[::-1][:15]\n",
        "  pymol_cmd=\"select ch\"+str(target_chain)+\",\"\n",
        "  for n,i in enumerate(top_n_idx):\n",
        "    p = pred_bind[i]\n",
        "    c = af_model._pdb[\"idx\"][\"chain\"][i]\n",
        "    r = af_model._pdb[\"idx\"][\"residue\"][i]\n",
        "    pymol_cmd += f\" resi {r}\"\n",
        "    if n < top_n-1:\n",
        "      pymol_cmd += \" +\"\n",
        "\n",
        "  print(\"\\nðŸ§ªPymol Selection Cmd:\")\n",
        "  print(pymol_cmd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ab5QehKkYyKB"
      },
      "outputs": [],
      "source": [
        "#@title **Display Structure** (Colored by Confidence)\n",
        "\n",
        "if large_pdb:\n",
        "\n",
        "  out_pdb_path = f\"{PREDS_PATH}/{target_pdb}/{target_pdb}_lumped_pred.pdb\"\n",
        "\n",
        "  hbondCutoff = 4.0\n",
        "\n",
        "  # Read the PDB file\n",
        "  with open(out_pdb_path, 'r') as file:\n",
        "      pdb_str = file.read()\n",
        "\n",
        "  # Initialize 3Dmol view\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js', width=800, height=600)\n",
        "  view.addModel(pdb_str, 'pdb', {'hbondCutoff': hbondCutoff})\n",
        "\n",
        "  # Define the color scheme for B-factors:\n",
        "  color_scheme = {'prop': 'b', 'gradient': 'linear', 'min': 0, 'max': 100, 'colors': ['white', 'blue']}\n",
        "  #color_scheme = {'prop': 'b', 'gradient': 'rwb', 'min': 0, 'max': 100}\n",
        "  view.setStyle({'cartoon': {'colorscheme': color_scheme}})\n",
        "  # Optional: Add sidechains if needed (this part is skipped as per your request)\n",
        "  # You can customize this section if you want to include specific residues later.\n",
        "\n",
        "  # Add hoverable labels showing B-factors\n",
        "  view.setHoverable({}, True,\n",
        "      '''function(atom,viewer,event,container){\n",
        "          if(!atom.label){\n",
        "              atom.label=viewer.addLabel(atom.chain+\"/\"+atom.resi+\"/\"+atom.resn+\" \"+(atom.b/100.0).toFixed(3),{\n",
        "                  position:atom,\n",
        "                  backgroundColor:'white',\n",
        "                  backgroundOpacity:0.75,\n",
        "                  borderColor:'black',\n",
        "                  borderThickness:2.0,\n",
        "                  fontColor:'black'\n",
        "              });\n",
        "          }\n",
        "      }''',\n",
        "      '''function(atom,viewer){\n",
        "          if(atom.label){\n",
        "              viewer.removeLabel(atom.label);\n",
        "              delete atom.label;\n",
        "          }\n",
        "      }'''\n",
        "  )\n",
        "\n",
        "  # Zoom to the view and display\n",
        "  view.zoomTo()\n",
        "  view.show()\n",
        "\n",
        "else:\n",
        "\n",
        "  use_native_coordinates = True\n",
        "  show_ligand = False\n",
        "\n",
        "\n",
        "  preds_adj = pred_bind.copy()\n",
        "\n",
        "  # replace plddt and coordinates of prediction\n",
        "  L = af_model._target_len\n",
        "  aux = copy.deepcopy(af_model.aux[\"all\"])\n",
        "  aux[\"plddt\"][:,:L] = preds_adj\n",
        "  if show_ligand:\n",
        "    af_model.save_pdb(\"output.pdb\",aux={\"all\":aux})\n",
        "  else:\n",
        "    aux[\"atom_mask\"][:,L:] = 0\n",
        "    x = {k:[] for k in [\"aatype\",\n",
        "                        \"residue_index\",\n",
        "                        \"atom_positions\",\n",
        "                        \"atom_mask\",\n",
        "                        \"b_factors\"]}\n",
        "    asym_id = []\n",
        "    for i in range(af_model._target_len):\n",
        "      for k in [\"aatype\",\"atom_mask\"]: x[k].append(aux[k][0,i])\n",
        "      if use_native_coordinates:\n",
        "        x[\"atom_positions\"].append(af_model._pdb[\"batch\"][\"all_atom_positions\"][i])\n",
        "      else:\n",
        "        x[\"atom_positions\"].append(aux[\"atom_positions\"][0,i])\n",
        "      x[\"residue_index\"].append(af_model._pdb[\"idx\"][\"residue\"][i])\n",
        "      x[\"b_factors\"].append(x[\"atom_mask\"][-1] * aux[\"plddt\"][0,i] * 100.0)\n",
        "      asym_id.append(af_model._pdb[\"idx\"][\"chain\"][i])\n",
        "    x = {k:np.array(v) for k,v in x.items()}\n",
        "\n",
        "    # fix the chains\n",
        "    (n,resnum_) = (0,None)\n",
        "    pdb_lines = []\n",
        "    for line in protein.to_pdb(protein.Protein(**x)).splitlines():\n",
        "      if line[:4] == \"ATOM\":\n",
        "        resnum = int(line[22:22+5])\n",
        "        if resnum_ is None: resnum_ = resnum\n",
        "        if resnum != resnum_:\n",
        "          n += 1\n",
        "          resnum_ = resnum\n",
        "        pdb_lines.append(\"%s%s%4i%s\" % (line[:21],asym_id[n],resnum,line[26:]))\n",
        "    with open(\"output.pdb\",\"w\") as handle:\n",
        "      handle.write(\"\\n\".join(pdb_lines))\n",
        "\n",
        "  hbondCutoff = 4.0\n",
        "  view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js',width=800,height=600)\n",
        "  pdb_str = open(\"output.pdb\",'r').read()\n",
        "  view.addModel(pdb_str,'pdb',{'hbondCutoff':hbondCutoff})\n",
        "  #color_scheme = {'prop':'b','gradient': 'rwb','min':0,'max':100}\n",
        "  color_scheme = {'prop': 'b', 'gradient': 'linear', 'min': 0, 'max': 100, 'colors': ['white', 'blue']}\n",
        "  view.setStyle({'cartoon': {'colorscheme': color_scheme}})\n",
        "\n",
        "  # add sidechains\n",
        "  for i in range(af_model._target_len):\n",
        "    c = af_model._pdb[\"idx\"][\"chain\"][i]\n",
        "    r = int(af_model._pdb[\"idx\"][\"residue\"][i])\n",
        "    p = pred_bind[i]\n",
        "    if p > 0.5:\n",
        "      view.addStyle({'and':[{'chain':c},{'resi':r},{'resn':[\"GLY\",\"PRO\"],'invert':True},{'atom':['C','O','N'],'invert':True}]},\n",
        "                    {'stick':{'colorscheme':color_scheme,'radius':0.3}})\n",
        "      view.addStyle({'and':[{'chain':c},{'resi':r},{'resn':\"GLY\"},{'atom':'CA'}]},\n",
        "                    {'sphere':{'colorscheme':color_scheme,'radius':0.3}})\n",
        "      view.addStyle({'and':[{'chain':c},{'resi':r},{'resn':\"PRO\"},{'atom':['C','O'],'invert':True}]},\n",
        "                    {'stick':{'colorscheme':color_scheme,'radius':0.3}})\n",
        "\n",
        "  view.setHoverable({}, True,\n",
        "                '''function(atom,viewer,event,container){if(!atom.label){atom.label=viewer.addLabel(atom.chain+\"/\"+atom.resi+\"/\"+atom.resn+\" \"+(atom.b/100.0).toFixed(3),{position:atom,backgroundColor:'white',backgroundOpacity:0.75,borderColor:'black',borderThickness:2.0,fontColor:'black'});}}''',\n",
        "                '''function(atom,viewer){if(atom.label){viewer.removeLabel(atom.label);delete atom.label;}}''')\n",
        "\n",
        "  view.zoomTo()\n",
        "  view.show()\n",
        "\n",
        "def plot_plddt_legend(dpi=100):\n",
        "  thresh = ['p(bind):','0.00','0.25','0.50','0.75','1.00']\n",
        "  plt.figure(figsize=(1,0.1),dpi=dpi)\n",
        "  ########################################\n",
        "  for c in ['#FDFDFD', '#CCE5FF', '#99CCFF',  '#66B3FF',  '#3399FF',  '#0000FF']:  #[\"white\",\"#FF0000\",\"#FF8080\",\"#FFFFFF\",\"#8080FF\",\"#0000FF\"]:\n",
        "    plt.bar(0, 0, color=c)\n",
        "  plt.legend(thresh, frameon=False,\n",
        "             loc='center', ncol=6,\n",
        "             handletextpad=1,\n",
        "             columnspacing=1,\n",
        "             markerscale=0.5,)\n",
        "  plt.axis(False)\n",
        "  return plt\n",
        "\n",
        "plot_plddt_legend().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "PlzGtGPzyeNB",
        "outputId": "4026e4a0-062c-4fad-9f29-292dff57191d"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_95a1b62b-4848-4704-9996-6b35f16e56d1\", \"output.zip\", 438855)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#@title **Download Predictions**\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "\n",
        "if(large_pdb):\n",
        "  with zipfile.ZipFile('output.zip', 'w') as zipf:\n",
        "    # Add files to the zip file\n",
        "    zipf.write(f\"{PREDS_PATH}/{target_pdb}/{target_pdb}_lumped_pred.pdb\",\n",
        "                arcname=f\"{target_pdb}_lumped_pred.pdb\")\n",
        "    zipf.write(f\"{PREDS_PATH}/{target_pdb}/domains_lumped_{target_pdb}.csv\",\n",
        "                arcname=f\"domains_lumped_{target_pdb}.csv\")\n",
        "\n",
        "  files.download(f'output.zip')\n",
        "else:\n",
        "  os.system(f\"zip -r output.zip output.pdb results.csv\")\n",
        "  files.download(f'output.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6LNbani6bCiz"
      },
      "outputs": [],
      "source": [
        "#@title **Activation analysis** (optional)\n",
        "#@markdown - **Not applicable to the large PDB pipeline.**\n",
        "\n",
        "pbind_cutoff = 0.5 # @param [\"0.0\", \"0.5\", \"0.9\"] {type:\"raw\"}\n",
        "blosum_map = list(\"CSTAGPDEQNHRKMILVWYF\")\n",
        "cs_label_list = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
        "\n",
        "indices_A_Y_mapping = np.array([cs_label_list.index(letter) for letter in blosum_map])\n",
        "pred_bind_aa_blosum = pred_bind_aa[:,indices_A_Y_mapping]\n",
        "filt = pred_bind > pbind_cutoff\n",
        "pred_bind_aa_blosum = pred_bind_aa_blosum[filt]\n",
        "res_labels = np.array(af_model._pdb[\"idx\"][\"residue\"])[filt]\n",
        "chain_labels = np.array(af_model._pdb[\"idx\"][\"chain\"])[filt]\n",
        "\n",
        "fig = px.imshow(pred_bind_aa_blosum.T,\n",
        "                labels=dict(x=\"positions\", y=\"amino acids\", color=\"pref\"),\n",
        "                y=blosum_map,\n",
        "                x=[f\"{y}_{x}\" for x,y in zip(res_labels,chain_labels)],\n",
        "                zmin=-1,\n",
        "                zmax=1,\n",
        "                template=\"simple_white\",\n",
        "                color_continuous_scale=[\"red\", \"white\", \"blue\"],\n",
        "              )\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
